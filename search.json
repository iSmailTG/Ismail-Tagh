[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Ismail-Tagh",
    "section": "",
    "text": "Post With Code\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nNov 28, 2024\n\n\nHarlow Malloc\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nNov 25, 2024\n\n\nTristan O’Malley\n\n\n\n\n\n\n\n\n\n\n\n\nChapter 1: Build a Large Language Model\n\n\n\n\n\n\nLLM\n\n\nDeep Learning\n\n\n\n\n\n\n\n\n\nNov 14, 2024\n\n\nIsmail TG\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/Build_A_Large_Language_Model_Chapter_1_.html",
    "href": "posts/Build_A_Large_Language_Model_Chapter_1_.html",
    "title": "Chapter 1: Build a Large Language Model",
    "section": "",
    "text": "In this series on notebooks I will share my learning/note-taking of the book Build a Large Lunguage Model(From Scratch) by the Author Sebastian Raschka.\n\n\n\n\n├── chapters  \n│   ├── chapter1_understanding_LLMs: high-level introduction to the fundamental concepts behind LLMs.  \n│   ├── chapter2_text_data: It covers the process ofpreparing text for LLM training, including splitting text into word and subword tokens.  \n│   ├── chapter3_attention_mechanisms:  It introduces a basicself-attention framework and progresses to an enhanced self-attention mechanism.  \n│   ├── chapter4_GPT_model: focuses on coding a GPT-like LLM that can be trained to generatehuman-like text.  \n│   ├── chapter5_pretraining: implements the pretraining process of LLMs.  \n│   ├── chapter6_text_classification:  introduces different LLM fine-tuning approaches.  \n│   ├── chapter7_instruction_following:  explores the instruction fine-tuning process of LLMs.  \n└──  \n\nThe aim of this chapter is to introduce the foundational concepts of large language models (LLMs) and the advancements in deep learning that made them possible\n\nthis chapter doesn’t contain any code.\n\nLarge language models (LLMs), like OpenAI’s ChatGPT, are deep neural networks that revolutionized natural language processing (NLP) in recent years.\nTraditional NLP methods excelled in tasks like spam classification and simple pattern recognition but struggled with complex tasks requiring advanced understanding and generation abilities.\nContemporary LLMs can handle sophisticated language tasks, such as writing an email from keywords, which was challenging for earlier models.\nWhen we say language models “understand,” we mean they can produce text that seems coherent and contextually appropriate, not that they have human-like awareness or true comprehension.\nThe transformer architecture and large datasets have driven the shift in NLP, enabling more advanced language understanding and interaction.\n\n\n\n\nLLM’s are neural network designed to understand and produce huma-like text.\nLarge in LLM refer to the size of the datasets those model trained on, but also on the size of parameters ( 100’s of billions)\n\nParameters are adjusted weights during training to predict next word in sentence.\n\nThe architecture of an LLM is called transformers which apply the attention mechanism to different parts of the input while performing the next word prediction. ### Applications of LLM’s:\nLLM’s can be used in many contexts to perform different tasks:\n\nmachine translation\nsentiments analysis\ntext generation\n..\n\n\n\n\n\n\nBuilding LLM form scratch allow us to understand the mechanics and limitations of language models, and provide us with skills set required for pretraining or fine-tuning phase.\nCustom-built LLM outperform general purpose one.\n\nMany companies prefer to build their own domain-specific llm to keep their private data in-home and not share it with third party.\ndeveloping small lm open the door for deployment on devices like laptops or even mobiles rather than huge servers.\n\ncreating LLM is a process where pre-training and fine-tuning takes place.\n\npre indicates that it is the first phase, model is trained on huge chunk of data where it learns basic knowledge and broad pattern of the language.\nthe fine-tuning phase is where the model get further training but on very specific task and get its knowledge narrowed.\n\nFine-tuning can be devised in 2 category:\n\nInstruction fine-tuning: where the model get trained one pair of instruction =&gt; output dataset.\nWhere classification tuning the data consist of text and associated class label.\n\n\n\n\n\n\nAll modern LLM rely on Transformer architecture which was presented for the first time in this famous paper: Attention is all you need.\nTransformer consist of two submodal: 1-encoder and 2-decoder. - encoder module process the input text into some numerical representation that capture meaning.\n- decoder uses the numerical values and generate text\n\nthe key component of the transformer architecture is attention mechanism, we will talk about it later.\nTransformer Variants:\n- Models like BERT and GPT are based on the original transformer architecture but adapt it for different tasks.\n- BERT’s Training Strategy: BERT uses a masked word prediction approach, where it predicts missing words in a sentence, making it suitable for tasks like text classification and sentiment analysis.\n- GPT vs. BERT: GPT is designed for generative tasks, whereas BERT excels in tasks requiring understanding of context, like sentiment prediction and document categorization.\n- BERT’s Real-world Application: Platforms like X (formerly Twitter) use BERT for tasks such as detecting toxic content.\nGPT Focus: GPT utilizes the Decoder portion of the transformer architecture and is designed for text generation tasks.\nZero-shot and Few-shot Learning: GPT models excel in zero-shot learning, meaning they can handle tasks without specific prior examples. They also perform well in few-shot learning, where they learn from a small number of provided examples.\nVersatility: While GPT models are optimized for text completion, they exhibit broad adaptability and can tackle a wide range of tasks, showcasing their flexibility in natural language processing.\n\n\n\n\n\nDiverse Training Data: Large datasets used for training GPT- and BERT-like models contain billions of words, covering a broad range of topics and languages (both natural and programming).\nComprehensive Corpus: These datasets are designed to ensure comprehensive exposure to diverse linguistic and contextual patterns.\n\n\n\n\n\nGPT Origin: GPT was introduced in the paper Improving Language Understanding by Generative Pre-Training by Radford et al. from OpenAI.\nGPT-3: A scaled-up version of the original GPT with more parameters and a larger training dataset.\nChatGPT’s Base Model: The initial ChatGPT model was derived by fine-tuning GPT-3 on a large instruction dataset, using methods from OpenAI’s InstructGPT paper.\nModel Versatility: Despite being trained on a simple next-word prediction task, GPT models excel in various tasks like text completion, spelling correction, classification, and language translation.\nSelf-Supervised Learning: The next-word prediction task is a type of self-supervised learning, where the model uses the structure of the data itself for training.\nLabel Creation: Labels are generated dynamically, with the next word in a sentence or document serving as the prediction target.\nTraining on Massive Datasets: This approach enables the use of large, unlabeled text datasets for training, as explicit labeling of data is unnecessary.\n\n\n\n\n\nNow we understand the basic theory behind LLM and how they were introduced, its time to build them from scratch.\n\n &gt;Source: Book: Build A Large Language Model by Sebastian Raschka"
  },
  {
    "objectID": "posts/Build_A_Large_Language_Model_Chapter_1_.html#structure-of-the-book",
    "href": "posts/Build_A_Large_Language_Model_Chapter_1_.html#structure-of-the-book",
    "title": "Chapter 1: Build a Large Language Model",
    "section": "",
    "text": "├── chapters  \n│   ├── chapter1_understanding_LLMs: high-level introduction to the fundamental concepts behind LLMs.  \n│   ├── chapter2_text_data: It covers the process ofpreparing text for LLM training, including splitting text into word and subword tokens.  \n│   ├── chapter3_attention_mechanisms:  It introduces a basicself-attention framework and progresses to an enhanced self-attention mechanism.  \n│   ├── chapter4_GPT_model: focuses on coding a GPT-like LLM that can be trained to generatehuman-like text.  \n│   ├── chapter5_pretraining: implements the pretraining process of LLMs.  \n│   ├── chapter6_text_classification:  introduces different LLM fine-tuning approaches.  \n│   ├── chapter7_instruction_following:  explores the instruction fine-tuning process of LLMs.  \n└──  \n\nThe aim of this chapter is to introduce the foundational concepts of large language models (LLMs) and the advancements in deep learning that made them possible\n\nthis chapter doesn’t contain any code.\n\nLarge language models (LLMs), like OpenAI’s ChatGPT, are deep neural networks that revolutionized natural language processing (NLP) in recent years.\nTraditional NLP methods excelled in tasks like spam classification and simple pattern recognition but struggled with complex tasks requiring advanced understanding and generation abilities.\nContemporary LLMs can handle sophisticated language tasks, such as writing an email from keywords, which was challenging for earlier models.\nWhen we say language models “understand,” we mean they can produce text that seems coherent and contextually appropriate, not that they have human-like awareness or true comprehension.\nThe transformer architecture and large datasets have driven the shift in NLP, enabling more advanced language understanding and interaction.\n\n\n\n\nLLM’s are neural network designed to understand and produce huma-like text.\nLarge in LLM refer to the size of the datasets those model trained on, but also on the size of parameters ( 100’s of billions)\n\nParameters are adjusted weights during training to predict next word in sentence.\n\nThe architecture of an LLM is called transformers which apply the attention mechanism to different parts of the input while performing the next word prediction. ### Applications of LLM’s:\nLLM’s can be used in many contexts to perform different tasks:\n\nmachine translation\nsentiments analysis\ntext generation\n..\n\n\n\n\n\n\nBuilding LLM form scratch allow us to understand the mechanics and limitations of language models, and provide us with skills set required for pretraining or fine-tuning phase.\nCustom-built LLM outperform general purpose one.\n\nMany companies prefer to build their own domain-specific llm to keep their private data in-home and not share it with third party.\ndeveloping small lm open the door for deployment on devices like laptops or even mobiles rather than huge servers.\n\ncreating LLM is a process where pre-training and fine-tuning takes place.\n\npre indicates that it is the first phase, model is trained on huge chunk of data where it learns basic knowledge and broad pattern of the language.\nthe fine-tuning phase is where the model get further training but on very specific task and get its knowledge narrowed.\n\nFine-tuning can be devised in 2 category:\n\nInstruction fine-tuning: where the model get trained one pair of instruction =&gt; output dataset.\nWhere classification tuning the data consist of text and associated class label.\n\n\n\n\n\n\nAll modern LLM rely on Transformer architecture which was presented for the first time in this famous paper: Attention is all you need.\nTransformer consist of two submodal: 1-encoder and 2-decoder. - encoder module process the input text into some numerical representation that capture meaning.\n- decoder uses the numerical values and generate text\n\nthe key component of the transformer architecture is attention mechanism, we will talk about it later.\nTransformer Variants:\n- Models like BERT and GPT are based on the original transformer architecture but adapt it for different tasks.\n- BERT’s Training Strategy: BERT uses a masked word prediction approach, where it predicts missing words in a sentence, making it suitable for tasks like text classification and sentiment analysis.\n- GPT vs. BERT: GPT is designed for generative tasks, whereas BERT excels in tasks requiring understanding of context, like sentiment prediction and document categorization.\n- BERT’s Real-world Application: Platforms like X (formerly Twitter) use BERT for tasks such as detecting toxic content.\nGPT Focus: GPT utilizes the Decoder portion of the transformer architecture and is designed for text generation tasks.\nZero-shot and Few-shot Learning: GPT models excel in zero-shot learning, meaning they can handle tasks without specific prior examples. They also perform well in few-shot learning, where they learn from a small number of provided examples.\nVersatility: While GPT models are optimized for text completion, they exhibit broad adaptability and can tackle a wide range of tasks, showcasing their flexibility in natural language processing.\n\n\n\n\n\nDiverse Training Data: Large datasets used for training GPT- and BERT-like models contain billions of words, covering a broad range of topics and languages (both natural and programming).\nComprehensive Corpus: These datasets are designed to ensure comprehensive exposure to diverse linguistic and contextual patterns.\n\n\n\n\n\nGPT Origin: GPT was introduced in the paper Improving Language Understanding by Generative Pre-Training by Radford et al. from OpenAI.\nGPT-3: A scaled-up version of the original GPT with more parameters and a larger training dataset.\nChatGPT’s Base Model: The initial ChatGPT model was derived by fine-tuning GPT-3 on a large instruction dataset, using methods from OpenAI’s InstructGPT paper.\nModel Versatility: Despite being trained on a simple next-word prediction task, GPT models excel in various tasks like text completion, spelling correction, classification, and language translation.\nSelf-Supervised Learning: The next-word prediction task is a type of self-supervised learning, where the model uses the structure of the data itself for training.\nLabel Creation: Labels are generated dynamically, with the next word in a sentence or document serving as the prediction target.\nTraining on Massive Datasets: This approach enables the use of large, unlabeled text datasets for training, as explicit labeling of data is unnecessary.\n\n\n\n\n\nNow we understand the basic theory behind LLM and how they were introduced, its time to build them from scratch.\n\n &gt;Source: Book: Build A Large Language Model by Sebastian Raschka"
  }
]